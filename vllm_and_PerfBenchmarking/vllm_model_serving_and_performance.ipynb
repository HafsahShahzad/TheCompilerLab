{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83a5fc21-32fe-47de-a792-7cee2e8254c4",
   "metadata": {},
   "source": [
    "# Running vllm - Model Inference and Comparing Performance\n",
    "\n",
    "\n",
    "### Some notes on vllm github repo structure: \n",
    "--> vllm/ → main Python API & high-level orchestration engine/ — request scheduling, batching logic\n",
    "\n",
    "--> model_executor/ — core model execution logic\n",
    "\n",
    "--> layers/ — includes paged_attention.py\n",
    "\n",
    "--> kv_cache/ — memory management for attention cache\n",
    "\n",
    "--> worker/ — GPU worker processes, distributed logic\n",
    "\n",
    "--> csrc/ → custom CUDA kernels (PagedAttention CUDA implementation lives here)\n",
    "\n",
    "--> examples/ → ready-to-run scripts\n",
    "\n",
    "--> benchmarks/ — perf testing scripts\n",
    "\n",
    "https://github.com/vllm-project/vllm/tree/main\n",
    "https://docs.nvidia.com/cutlass/media/docs/cpp/efficient_gemm.html\n",
    "https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a9d8814-d244-4d72-816d-ef50dfd20690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug 12 17:45:16 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4070        Off |   00000000:41:00.0 Off |                  N/A |\n",
      "|  0%   36C    P8             10W /  215W |      36MiB /  12282MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1934      G   /usr/libexec/Xorg                              16MiB |\n",
      "|    0   N/A  N/A      2075      G   /usr/bin/gnome-shell                            5MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 4070\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e12c761e-25d7-4b21-ab7b-511ead1b304b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hshahzad/vllm/env_vllm/lib64/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:45:20 [__init__.py:235] Automatically detected platform cuda.\n",
      "GPU: NVIDIA GeForce RTX 4070\n",
      "Total memory: 12.88 GB\n",
      "Memory bandwidth: 288.0  GB/s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "##known warning from HuggingFace tokenizers when using multiprocessing \n",
    "#(like in Jupyter or multiprocessing environments). It doesn’t break your code but can be silenced with setting this flag\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pynvml\n",
    "\n",
    "##Initialize NVML to query GPU info\n",
    "pynvml.nvmlInit()\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)  # Assuming single GPU\n",
    "\n",
    "##Query GPU properties\n",
    "gpu_name = pynvml.nvmlDeviceGetName(handle)\n",
    "mem_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "mem_bandwidth_bytes = 288 * 1e9  # RTX 4070 approx. bandwidth in bytes/sec\n",
    "\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Total memory: {mem_info.total / 1e9:.2f} GB\")\n",
    "print(f\"Memory bandwidth: {mem_bandwidth_bytes/1e9:.1f}  GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5518781d-5039-47d1-908e-f9e69a3f1401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:45:27 [config.py:1604] Using max model len 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 17:45:28,898\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:45:28 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 08-12 17:45:29 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 08-12 17:45:32 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-12 17:45:33 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-12 17:45:33 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-12 17:45:35 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-12 17:45:35 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-12 17:45:35 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 08-12 17:45:35 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-12 17:45:35 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-12 17:45:36 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 08-12 17:45:36 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.24it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.23it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:45:36 [default_loader.py:262] Loading weights took 0.53 seconds\n",
      "INFO 08-12 17:45:37 [gpu_model_runner.py:1892] Model loading took 2.8876 GiB and 1.157818 seconds\n",
      "INFO 08-12 17:45:42 [backends.py:530] Using cache directory: /home/hshahzad/.cache/vllm/torch_compile_cache/28339d7bf3/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-12 17:45:42 [backends.py:541] Dynamo bytecode transform time: 5.33 s\n",
      "INFO 08-12 17:45:46 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.046 s\n",
      "INFO 08-12 17:45:47 [monitor.py:34] torch.compile takes 5.33 s in total\n",
      "INFO 08-12 17:45:48 [gpu_worker.py:255] Available KV cache memory: 2.59 GiB\n",
      "INFO 08-12 17:45:48 [kv_cache_utils.py:833] GPU KV cache size: 97,120 tokens\n",
      "INFO 08-12 17:45:48 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 2.96x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:01<00:00, 35.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:45:51 [gpu_model_runner.py:2485] Graph capturing finished in 2 secs, took 0.46 GiB\n",
      "INFO 08-12 17:45:51 [core.py:193] init engine (profile, create kv cache, warmup model) took 13.98 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 853.19it/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.52it/s, est. speed input: 22.83 toks/s, output: 126.83 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". TMA is a new instruction set for GPUs, designed to optimize the performance of machine learning and deep learning tasks. It stands for Tensor Math Accelerator and is inspired by the popular Tensorflow instruction set.\n",
      "TMA is optimized for the use of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W812 17:45:52.236053464 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "# Here we do an example model serving with vllm\n",
    "# note I have 12GB on my gpu so cannot run bigger models. \n",
    "#https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    gpu_memory_utilization=0.6  # adjust this parameter--\n",
    ")\n",
    "params = SamplingParams(temperature=0.7, max_tokens=50)\n",
    "outputs = llm.generate([\"Explain TMA in hopper gpus\"], params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output.outputs[0].text)\n",
    "del llm  # delete your vLLM model object\n",
    "torch.cuda.empty_cache()  # clear PyTorch GPU cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62e5ef3-1404-4909-b0cf-ba6ac1ec3241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:45:53 [config.py:1604] Using max model len 32768\n",
      "INFO 08-12 17:45:53 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-12 17:45:57 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-12 17:45:58 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-12 17:45:58 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='Qwen/Qwen2.5-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-12 17:45:59 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-12 17:45:59 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-12 17:45:59 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen2.5-1.5B-Instruct...\n",
      "INFO 08-12 17:46:00 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-12 17:46:00 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-12 17:46:00 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 08-12 17:46:00 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  2.22it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:46:00 [default_loader.py:262] Loading weights took 0.53 seconds\n",
      "INFO 08-12 17:46:01 [gpu_model_runner.py:1892] Model loading took 2.8876 GiB and 0.798807 seconds\n",
      "INFO 08-12 17:46:06 [backends.py:530] Using cache directory: /home/hshahzad/.cache/vllm/torch_compile_cache/28339d7bf3/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-12 17:46:06 [backends.py:541] Dynamo bytecode transform time: 5.41 s\n",
      "INFO 08-12 17:46:11 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.118 s\n",
      "INFO 08-12 17:46:11 [monitor.py:34] torch.compile takes 5.41 s in total\n",
      "INFO 08-12 17:46:12 [gpu_worker.py:255] Available KV cache memory: 2.59 GiB\n",
      "INFO 08-12 17:46:13 [kv_cache_utils.py:833] GPU KV cache size: 97,120 tokens\n",
      "INFO 08-12 17:46:13 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 2.96x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:01<00:00, 35.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:46:15 [gpu_model_runner.py:2485] Graph capturing finished in 2 secs, took 0.46 GiB\n",
      "INFO 08-12 17:46:15 [core.py:193] init engine (profile, create kv cache, warmup model) took 14.13 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 1463.98it/s]\n",
      "Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  2.55it/s, est. speed input: 20.52 toks/s, output: 128.22 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". TMA is a new instruction set for Hopper GPUs that provides a more efficient way to handle data in parallel. Instead of storing data in a linear fashion, TMA stores data in a tree-like structure, which allows for more efficient sharing of\n",
      "Model Qwen/Qwen2.5-1.5B-Instruct inference time: 0.396 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W812 17:46:16.468528867 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:46:17 [config.py:1604] Using max model len 32768\n",
      "INFO 08-12 17:46:19 [awq_marlin.py:116] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 08-12 17:46:19 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 08-12 17:46:22 [__init__.py:235] Automatically detected platform cuda.\n",
      "INFO 08-12 17:46:24 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 08-12 17:46:24 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='Qwen/Qwen2.5-1.5B-Instruct-AWQ', speculative_config=None, tokenizer='Qwen/Qwen2.5-1.5B-Instruct-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-1.5B-Instruct-AWQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 08-12 17:46:25 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 08-12 17:46:25 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 08-12 17:46:25 [gpu_model_runner.py:1843] Starting to load model Qwen/Qwen2.5-1.5B-Instruct-AWQ...\n",
      "INFO 08-12 17:46:25 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "INFO 08-12 17:46:25 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "INFO 08-12 17:46:25 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "INFO 08-12 17:46:25 [weight_utils.py:349] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.39it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.38it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:46:26 [default_loader.py:262] Loading weights took 0.26 seconds\n",
      "INFO 08-12 17:46:26 [gpu_model_runner.py:1892] Model loading took 1.1002 GiB and 0.659654 seconds\n",
      "INFO 08-12 17:46:33 [backends.py:530] Using cache directory: /home/hshahzad/.cache/vllm/torch_compile_cache/adf68c476f/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 08-12 17:46:33 [backends.py:541] Dynamo bytecode transform time: 6.11 s\n",
      "INFO 08-12 17:46:38 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.532 s\n",
      "INFO 08-12 17:46:39 [monitor.py:34] torch.compile takes 6.11 s in total\n",
      "INFO 08-12 17:46:40 [gpu_worker.py:255] Available KV cache memory: 4.38 GiB\n",
      "INFO 08-12 17:46:40 [kv_cache_utils.py:833] GPU KV cache size: 164,048 tokens\n",
      "INFO 08-12 17:46:40 [kv_cache_utils.py:837] Maximum concurrency for 32,768 tokens per request: 5.01x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:01<00:00, 36.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-12 17:46:42 [gpu_model_runner.py:2485] Graph capturing finished in 2 secs, took 0.48 GiB\n",
      "INFO 08-12 17:46:42 [core.py:193] init engine (profile, create kv cache, warmup model) took 15.95 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 2075.36it/s]\n",
      "Processed prompts:   0%|                                                                                                                       | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    }
   ],
   "source": [
    "## Measure Performance of the model with its quantized version (Quantized using AWQ- model downloaded from internet)\n",
    "\n",
    "def measure_latency(model_name, prompt, max_tokens=50):\n",
    "    llm = LLM(\n",
    "        model=model_name,\n",
    "        trust_remote_code=True,\n",
    "        gpu_memory_utilization=0.6,\n",
    "    )\n",
    "    params = SamplingParams(temperature=0.7, max_tokens=max_tokens)\n",
    "\n",
    "    start_time = time.time()\n",
    "    outputs = llm.generate([prompt], params)\n",
    "    end_time = time.time()\n",
    "\n",
    "    for output in outputs:\n",
    "        print(output.outputs[0].text)\n",
    "\n",
    "    llm_time = end_time - start_time\n",
    "    print(f\"Model {model_name} inference time: {llm_time:.3f} seconds\")\n",
    "\n",
    "    del llm\n",
    "    import torch\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return llm_time\n",
    "\n",
    "prompt = \"Explain TMA in Hopper GPUs\"\n",
    "\n",
    "\n",
    "unquantized_model = \"Qwen/Qwen2.5-1.5B-Instruct\" \n",
    "quantized_model = \"Qwen/Qwen2.5-1.5B-Instruct-AWQ\"\n",
    "\n",
    "unquantized_time = measure_latency(unquantized_model, prompt)\n",
    "quantized_time = measure_latency(quantized_model, prompt)\n",
    "\n",
    "print(f\"Unquantized model time: {unquantized_time:.3f}s\")\n",
    "print(f\"Quantized model time: {quantized_time:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a9f502-11e8-4723-a2be-120de7f3b0d4",
   "metadata": {},
   "source": [
    "## Some Notes:\n",
    "\n",
    "So when I run the above, I notice my VRAM on GPU is still occupied. Running another command gives error that not enough memory. This happens because:\n",
    "--> Memory caching / pooling:Frameworks (like PyTorch underneath vLLM) keep allocated VRAM to speed up subsequent operations. They don't free GPU memory immediately to avoid costly reallocations later.\n",
    "--> Persistent model & buffers:\n",
    "Your loaded model, KV caches, buffers, CUDA contexts all remain in GPU memory until you explicitly delete or exit.\n",
    "-->Background processes:\n",
    "vLLM's engine might be running background workers or processes keeping memory reserved.\n",
    "\n",
    "This is why we have added del llm and torch.cuda.empty_cache(). You can keep checking using nvidia-smi if any processes runninga nd using up memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f923266-83df-4870-b817-29cf0b485e00",
   "metadata": {},
   "source": [
    "## Performance Monitoring with Nvidia Nsight\n",
    "\n",
    "(env_vllm) hshahzad@codes-1:~/GPU_work/Tests$ nsys profile --output ./<name>.nsys-rep --force-overwrite true vllm serve Qwen/Qwen2.5-1.5B-Instruct-AWQ\n",
    "\n",
    "In a separate terminal (no venv needed):\n",
    "$ curl -X POST http://localhost:8000/v1/completions   -H \"Content-Type: application/json\"   -d '{\n",
    "        \"model\": \"Qwen/Qwen2.5-1.5B-Instruct-AWQ\",\n",
    "        \"prompt\": \"Explain TMA in hopper gpus\",\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.7\n",
    "      }'\n",
    "\n",
    "--> once response recieved, terminate vllm server using Ctrl+C (ONCE) in first terminal\n",
    "--> Wait for report generation to complete\n",
    "\n",
    "--> run nsys stats <name>.nsys-rep\n",
    "--> Or save all info in textfile: nsys stats <name>.nsys-rep > quant.txt\n",
    "\n",
    "--> On ur desktop u can run Nvidia Nsight Systems application(its in Applications-> Programming) Once launched go to: ->File->Open-><name>.nsys rep\n",
    "--> this way you can visualize the timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a0abb2-05e9-40d5-8ddf-79c38948676a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measuring Latency and Throughput with Python Profiler\n",
    "## Query GPU memory usage before and after running inference using nvidia-smi programmatically or like in this case, PyTorch's CUDA API:\n",
    "## What we measure here:\n",
    "#-->load_time_s → Model load time (sec)\n",
    "#-->gpu_mem_mb → Actual GPU memory usage (MB) after load\n",
    "#-->gen_time_s → Time to generate output for given prompt\n",
    "#-->tokens_generated → Output length in tokens\n",
    "#-->throughput_tok_s → Tokens per second during generation\n",
    "\n",
    "\n",
    "import time\n",
    "import pynvml\n",
    "import pandas as pd\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "## --** GPU Memory Utility **--\n",
    "def get_gpu_memory_mb(gpu_index=0):\n",
    "    pynvml.nvmlInit()\n",
    "    handle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\n",
    "    info = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "    return info.used / 1024**2  # MB\n",
    "\n",
    "## --** Benchmark Function **--\n",
    "def benchmark_model(model_path, prompt, quantized=False, runs=3):\n",
    "    print(f\"\\n Loading model: {model_path} {'(Quantized)' if quantized else '(Unquantized)'}\")\n",
    "    start_load = time.time()\n",
    "    llm = LLM(model=model_path)\n",
    "    load_time = time.time() - start_load\n",
    "    mem_after_load = get_gpu_memory_mb()\n",
    "\n",
    "    sampling_params = SamplingParams(temperature=0.0, max_tokens=128)\n",
    "\n",
    "    gen_times = []\n",
    "    throughputs = []\n",
    "    tokens_counts = []\n",
    "\n",
    "    for i in range(runs):\n",
    "        start_gen = time.time()\n",
    "        output = llm.generate([prompt], sampling_params=sampling_params)\n",
    "        gen_time = time.time() - start_gen\n",
    "\n",
    "        tokens_generated = len(output[0].outputs[0].token_ids)\n",
    "        throughput = tokens_generated / gen_time\n",
    "\n",
    "        gen_times.append(gen_time)\n",
    "        throughputs.append(throughput)\n",
    "        tokens_counts.append(tokens_generated)\n",
    "\n",
    "    return {\n",
    "        \"model\": model_path,\n",
    "        \"quantized\": quantized,\n",
    "        \"load_time_s\": load_time,\n",
    "        \"gpu_mem_mb\": mem_after_load,\n",
    "        \"avg_gen_time_s\": sum(gen_times) / runs,\n",
    "        \"avg_tokens_generated\": sum(tokens_counts) / runs,\n",
    "        \"avg_throughput_tok_s\": sum(throughputs) / runs\n",
    "    }\n",
    "\n",
    "# --** Test Settings **--\n",
    "prompt = \"Explain architecture of Blackwell GPUs.\"\n",
    "runs_per_model = 5\n",
    "\n",
    "# --** Run Benchmarks **--\n",
    "results = []\n",
    "results.append(benchmark_model(\"Qwen/Qwen2.5-1.5B-Instruct\", prompt, quantized=False, runs=runs_per_model))\n",
    "results.append(benchmark_model(\"Qwen/Qwen2.5-1.5B-Instruct-AWQ\", prompt, quantized=True, runs=runs_per_model))\n",
    "\n",
    "# --** Show Results **--\n",
    "df = pd.DataFrame(results)\n",
    "print(\"\\n Final Benchmark Results (Averaged over\", runs_per_model, \"runs)\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23559b9-3db2-47df-b5f6-94f8e2b4358f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env_vllm)",
   "language": "python",
   "name": "env_vllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
